{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databased used\n",
    "- CREMA-D: 7,442 clips\n",
    "- RAVDESS: 2,880 clips\n",
    "- TESS: 2,000 clips\n",
    "- SAVEE: 480 clips\n",
    "\n",
    "This provides a total of **12,802** inputs overall, with 7 emotions (anger, disgust, fear, happy, neutral, sad, suprise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# KERNEL SETUP IN VS CODE:\n",
    "# conda create -n myenv python=3.12.2\n",
    "# conda activate myenv\n",
    "\n",
    "%pip install resampy tf_keras tensorflow librosa pandas matplotlib kagglehub seaborn\n",
    "\n",
    "import IPython.display as ipd\n",
    "from IPython.display import Audio\n",
    "import kagglehub\n",
    "import librosa\n",
    "from librosa import feature\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets via kagglehub\n",
    "\n",
    "cremad = kagglehub.dataset_download(\"ejlok1/cremad\")\n",
    "print(\"CREMA-D to dataset files:\", cremad)\n",
    "\n",
    "ravdess = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
    "print(\"RAVDESS to dataset files:\", ravdess)\n",
    "\n",
    "tess = kagglehub.dataset_download(\"ejlok1/toronto-emotional-speech-set-tess\")\n",
    "print(\"RAVDESS to dataset files:\", tess)\n",
    "\n",
    "savee = kagglehub.dataset_download(\"ejlok1/surrey-audiovisual-expressed-emotion-savee\")\n",
    "print(\"RAVDESS to dataset files:\", savee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "## CREMA-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CREMA-D Dataset\n",
    "paths = []\n",
    "emotions = []\n",
    "\n",
    "for dirname, _, filenames in os.walk(cremad): # (dirname, subdirs, filenames)\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.wav'):\n",
    "            paths.append(os.path.join(dirname, filename))\n",
    "            emotion = filename.split('_')[2]  # Get the emotion code (e.g., 'ANG')\n",
    "            emotions.append(emotion)\n",
    "\n",
    "print(paths[:5])\n",
    "\n",
    "print(emotions[:5])\n",
    "\n",
    "# Create DataFrame\n",
    "cremad_df = pd.DataFrame()\n",
    "cremad_df['paths'] = paths\n",
    "cremad_df['emotions'] = emotions\n",
    "\n",
    "# Map emotion codes to full emotions\n",
    "emotion_map = {\n",
    "    'ANG': 'anger',\n",
    "    'DIS': 'disgust',\n",
    "    'FEA': 'fear',\n",
    "    'HAP': 'happy',\n",
    "    'NEU': 'neutral',\n",
    "    'SAD': 'sad'\n",
    "}\n",
    "\n",
    "cremad_df['emotions'] = cremad_df['emotions'].map(emotion_map)\n",
    "\n",
    "print(cremad_df.head())\n",
    "\n",
    "print(cremad_df['emotions'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAVEDESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "emotions = []\n",
    "\n",
    "for dirname, _, filenames in os.walk(ravdess): # (dirname, subdirs, filenames)\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.wav'):\n",
    "            paths.append(os.path.join(dirname, filename))\n",
    "            part = filename.split('.')[0].split('-')  # Get the emotion number (e.g., '03' = happy)\n",
    "            emotions.append(int(part[2]))\n",
    "\n",
    "\n",
    "print(paths[:5])\n",
    "print(emotions[:5])\n",
    "\n",
    "# Create DataFrame\n",
    "ravdess_df = pd.DataFrame()\n",
    "ravdess_df['paths'] = paths\n",
    "ravdess_df['emotions'] = emotions\n",
    "\n",
    "\n",
    "# Map emotion codes to full emotions\n",
    "emotion_map = {\n",
    "    1 : 'neutral',\n",
    "    2 : 'neutral', # calm as neutral to balance dataset\n",
    "    3 : 'happy',\n",
    "    4 : 'sad',\n",
    "    5 : 'anger',\n",
    "    6 : 'fear',\n",
    "    7 : 'disgust',\n",
    "    8 : 'suprise'\n",
    "\n",
    "}\n",
    "\n",
    "ravdess_df['emotions'] = ravdess_df['emotions'].map(emotion_map)\n",
    "\n",
    "print(ravdess_df.head())\n",
    "\n",
    "print(ravdess_df['emotions'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "emotions = []\n",
    "\n",
    "for dirname, _, filenames in os.walk(tess): # (dirname, subdirs, filenames)\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.wav'):\n",
    "            paths.append(os.path.join(dirname, filename))\n",
    "            emotion = filename.split('.')[0].split('_')[2]  # Get the emotion code (e.g., 'ANG')\n",
    "            emotions.append(emotion)\n",
    "\n",
    "print(paths[:5])\n",
    "\n",
    "print(emotions[:5])\n",
    "\n",
    "# Create DataFrame\n",
    "tess_df = pd.DataFrame()\n",
    "tess_df['paths'] = paths\n",
    "tess_df['emotions'] = emotions\n",
    "\n",
    "# Map emotion codes to full emotions\n",
    "emotion_map = {\n",
    "    'angry': 'anger',\n",
    "    'disgust': 'disgust',\n",
    "    'fear': 'fear',\n",
    "    'happy': 'happy',\n",
    "    'neutral': 'neutral',\n",
    "    'ps' : 'suprise',\n",
    "    'sad' : 'sad'\n",
    "}\n",
    "\n",
    "tess_df['emotions'] = tess_df['emotions'].map(emotion_map)\n",
    "\n",
    "print(tess_df.head())\n",
    "\n",
    "print(tess_df['emotions'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\eskillman\\\\.cache\\\\kagglehub\\\\datasets\\\\ejlok1\\\\surrey-audiovisual-expressed-emotion-savee\\\\versions\\\\1\\\\ALL\\\\DC_a01.wav', 'C:\\\\Users\\\\eskillman\\\\.cache\\\\kagglehub\\\\datasets\\\\ejlok1\\\\surrey-audiovisual-expressed-emotion-savee\\\\versions\\\\1\\\\ALL\\\\DC_a02.wav', 'C:\\\\Users\\\\eskillman\\\\.cache\\\\kagglehub\\\\datasets\\\\ejlok1\\\\surrey-audiovisual-expressed-emotion-savee\\\\versions\\\\1\\\\ALL\\\\DC_a03.wav', 'C:\\\\Users\\\\eskillman\\\\.cache\\\\kagglehub\\\\datasets\\\\ejlok1\\\\surrey-audiovisual-expressed-emotion-savee\\\\versions\\\\1\\\\ALL\\\\DC_a04.wav', 'C:\\\\Users\\\\eskillman\\\\.cache\\\\kagglehub\\\\datasets\\\\ejlok1\\\\surrey-audiovisual-expressed-emotion-savee\\\\versions\\\\1\\\\ALL\\\\DC_a05.wav']\n",
      "['a', 'a', 'a', 'a', 'a']\n",
      "                                               paths emotions\n",
      "0  C:\\Users\\eskillman\\.cache\\kagglehub\\datasets\\e...    anger\n",
      "1  C:\\Users\\eskillman\\.cache\\kagglehub\\datasets\\e...    anger\n",
      "2  C:\\Users\\eskillman\\.cache\\kagglehub\\datasets\\e...    anger\n",
      "3  C:\\Users\\eskillman\\.cache\\kagglehub\\datasets\\e...    anger\n",
      "4  C:\\Users\\eskillman\\.cache\\kagglehub\\datasets\\e...    anger\n",
      "emotions\n",
      "neutral    120\n",
      "disgust     60\n",
      "anger       60\n",
      "fear        60\n",
      "happy       60\n",
      "sad         60\n",
      "suprise     60\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "paths = []\n",
    "emotions = []\n",
    "\n",
    "for dirname, _, filenames in os.walk(savee): # (dirname, subdirs, filenames)\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.wav'):\n",
    "            paths.append(os.path.join(dirname, filename))\n",
    "            part = filename.split('_')[1]  # Get the emotion code (e.g., 'ANG')\n",
    "            emotion = part[:-6]\n",
    "            emotions.append(emotion)\n",
    "\n",
    "print(paths[:5])\n",
    "\n",
    "print(emotions[:5])\n",
    "\n",
    "# Create DataFrame\n",
    "savee_df = pd.DataFrame()\n",
    "savee_df['paths'] = paths\n",
    "savee_df['emotions'] = emotions\n",
    "\n",
    "emotion_map = {\n",
    "    'n': 'neutral',\n",
    "    'd': 'disgust',\n",
    "    'a': 'anger',\n",
    "    'f': 'fear',\n",
    "    'h': 'happy',\n",
    "    'sa': 'sad',\n",
    "    'su' : 'suprise'\n",
    "}\n",
    "\n",
    "savee_df['emotions'] = savee_df['emotions'].map(emotion_map)\n",
    "\n",
    "print(savee_df.head())\n",
    "\n",
    "print(savee_df['emotions'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_data = pd.concat([cremad_df, ravdess_df, tess_df, savee_df], axis = 0)\n",
    "\n",
    "emotion_data.to_csv(\"emotion_data.csv\", index=False)\n",
    "\n",
    "print(emotion_data.emotions.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.title('Emotions Count', size=16)\n",
    "sns.countplot(emotion_data.emotions)\n",
    "plt.xlabel('Count', size=12)\n",
    "plt.ylabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sr = librosa.load(paths[0], sr=None) #Latest path value from SAVEE (angry)\n",
    "print(emotions[0])\n",
    "ipd.Audio(data,rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 128\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "fmax = 8000\n",
    "mel_spectogram = librosa.feature.melspectrogram(y=data, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, fmax=fmax)\n",
    "\n",
    "log_mel_spectogram = librosa.power_to_db(mel_spectogram)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.specshow(log_mel_spectogram, x_axis='time', y_axis='mel', sr=sr)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel Spectogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=30)\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.subplot(3,1,1)\n",
    "librosa.display.specshow(mfcc, x_axis='time')\n",
    "plt.ylabel('MFCC')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Stretching\n",
    "def stretch(audio, rate=0.8):\n",
    "    return librosa.effects.time_stretch(audio, rate=0.8)\n",
    "\n",
    "# Pitch Shifting\n",
    "def pitch(audio, sr):\n",
    "    return librosa.effects.pitch_shift(y=audio, sr=sr, n_steps=2)\n",
    "\n",
    "# Add Noise\n",
    "def noise(audio, noise_level=0.005):\n",
    "    noise_amp = noise_level * np.amax(data)\n",
    "    audio = audio + noise_amp * np.random.normal(0, 1, len(data))\n",
    "    return audio\n",
    "\n",
    "# Shifting (Time warping)\n",
    "def shift(audio):\n",
    "    return np.roll(audio, shift=int(sr * 0.2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal\n",
    "import librosa.display\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.waveshow(y=data, sr=sr)\n",
    "ipd.Audio(data,rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Stretching\n",
    "x = stretch(data)\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitch Shifting\n",
    "x = pitch(data, sr)\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Noise\n",
    "x = noise(data)\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shifting (Time warping)\n",
    "x = shift(data)\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 128, 393)\n"
     ]
    }
   ],
   "source": [
    "n_mels = 128\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "fmax = 8000\n",
    "\n",
    "features = []\n",
    "\n",
    "def extract_features(path, sr=None):\n",
    "\n",
    "    audio, sr = librosa.load(path, sr=sr)\n",
    "\n",
    "    augmented_audio = [ audio, shift(audio), noise(audio), pitch(audio, sr), stretch(audio) ]\n",
    "    \n",
    "    max_time_steps = 0\n",
    "    \n",
    "    for data in augmented_audio:\n",
    "\n",
    "        # Find max feature time step\n",
    "        for data in augmented_audio:\n",
    "            mel_spectogram = librosa.feature.melspectrogram(y=data, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, fmax=fmax)\n",
    "            log_mel_spectogram = librosa.power_to_db(mel_spectogram)\n",
    "            max_time_steps = max(max_time_steps, log_mel_spectogram.shape[1])\n",
    "\n",
    "        # MFCC\n",
    "        # mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=30)\n",
    "\n",
    "        # time_steps = min(log_mel_spectogram.shape[1], mfcc.shape[1])\n",
    "        # log_mel_spectogram = log_mel_spectogram[:, :time_steps]\n",
    "        # mfcc = mfcc[:, :time_steps]\n",
    "\n",
    "        for data in augmented_audio:\n",
    "            mel_spectogram = librosa.feature.melspectrogram(y=data, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, fmax=fmax)\n",
    "            log_mel_spectogram = librosa.power_to_db(mel_spectogram)\n",
    "\n",
    "            if log_mel_spectogram.shape[1] < max_time_steps:\n",
    "                pad_width = max_time_steps - log_mel_spectogram.shape[1]\n",
    "                log_mel_spectogram = np.pad(log_mel_spectogram, ((0,0), (0, pad_width)), mode='constant')\n",
    "            elif log_mel_spectogram.shape[1] > max_time_steps:\n",
    "                log_mel_spectogram = log_mel_spectogram[:, :max_time_steps]\n",
    "\n",
    "        features.append(log_mel_spectogram)\n",
    "\n",
    "    np_features = np.array(features)\n",
    "\n",
    "    return np_features\n",
    "\n",
    "np_features = extract_features(paths[0])\n",
    "print(np_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  8\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "print(\"Number of processors: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Run feature extraction in parallel\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m---> 13\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_paths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Convert to NumPy array and remove None values (if any files failed)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m])\n",
      "File \u001b[1;32mc:\\Users\\eskillman\\.conda\\envs\\myenv\\Lib\\concurrent\\futures\\process.py:642\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[1;34m(iterable)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[0;32m    637\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 642\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43melement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eskillman\\.conda\\envs\\myenv\\Lib\\concurrent\\futures\\_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32mc:\\Users\\eskillman\\.conda\\envs\\myenv\\Lib\\concurrent\\futures\\_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32mc:\\Users\\eskillman\\.conda\\envs\\myenv\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\eskillman\\.conda\\envs\\myenv\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
