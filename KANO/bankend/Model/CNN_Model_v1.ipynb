{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databased used\n",
    "- CREMA-D: 7,442 clips\n",
    "- RAVDESS: 2,880 clips\n",
    "- TESS: 2,000 clips\n",
    "- SAVEE: 480 clips\n",
    "\n",
    "This provides a total of **12,802** inputs overall, with 7 emotions (anger, disgust, fear, happy, neutral, sad, suprise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# KERNEL SETUP IN VS CODE:\n",
    "# conda create -n myenv python=3.12.2\n",
    "# conda activate myenv\n",
    "\n",
    "%pip install resampy tf_keras tensorflow librosa pandas matplotlib kagglehub seaborn\n",
    "\n",
    "import IPython.display as ipd\n",
    "from IPython.display import Audio\n",
    "import kagglehub\n",
    "import librosa\n",
    "from librosa import feature\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets via kagglehub\n",
    "\n",
    "cremad = kagglehub.dataset_download(\"ejlok1/cremad\")\n",
    "print(\"CREMA-D to dataset files:\", cremad)\n",
    "\n",
    "ravdess = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
    "print(\"RAVDESS to dataset files:\", ravdess)\n",
    "\n",
    "tess = kagglehub.dataset_download(\"ejlok1/toronto-emotional-speech-set-tess\")\n",
    "print(\"RAVDESS to dataset files:\", tess)\n",
    "\n",
    "savee = kagglehub.dataset_download(\"ejlok1/surrey-audiovisual-expressed-emotion-savee\")\n",
    "print(\"RAVDESS to dataset files:\", savee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "## CREMA-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CREMA-D Dataset\n",
    "paths = []\n",
    "emotions = []\n",
    "\n",
    "for dirname, _, filenames in os.walk(cremad): # (dirname, subdirs, filenames)\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.wav'):\n",
    "            paths.append(os.path.join(dirname, filename))\n",
    "            emotion = filename.split('_')[2]  # Get the emotion code (e.g., 'ANG')\n",
    "            emotions.append(emotion)\n",
    "\n",
    "print(paths[:5])\n",
    "\n",
    "print(emotions[:5])\n",
    "\n",
    "# Create DataFrame\n",
    "cremad_df = pd.DataFrame()\n",
    "cremad_df['paths'] = paths\n",
    "cremad_df['emotions'] = emotions\n",
    "\n",
    "# Map emotion codes to full emotions\n",
    "emotion_map = {\n",
    "    'ANG': 'anger',\n",
    "    'DIS': 'disgust',\n",
    "    'FEA': 'fear',\n",
    "    'HAP': 'happy',\n",
    "    'NEU': 'neutral',\n",
    "    'SAD': 'sad'\n",
    "}\n",
    "\n",
    "cremad_df['emotions'] = cremad_df['emotions'].map(emotion_map)\n",
    "\n",
    "print(cremad_df.head())\n",
    "\n",
    "print(cremad_df['emotions'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAVEDESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "emotions = []\n",
    "\n",
    "for dirname, _, filenames in os.walk(ravdess): # (dirname, subdirs, filenames)\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.wav'):\n",
    "            paths.append(os.path.join(dirname, filename))\n",
    "            part = filename.split('.')[0].split('-')  # Get the emotion number (e.g., '03' = happy)\n",
    "            emotions.append(int(part[2]))\n",
    "\n",
    "\n",
    "print(paths[:5])\n",
    "print(emotions[:5])\n",
    "\n",
    "# Create DataFrame\n",
    "ravdess_df = pd.DataFrame()\n",
    "ravdess_df['paths'] = paths\n",
    "ravdess_df['emotions'] = emotions\n",
    "\n",
    "\n",
    "# Map emotion codes to full emotions\n",
    "emotion_map = {\n",
    "    1 : 'neutral',\n",
    "    2 : 'neutral', # calm as neutral to balance dataset\n",
    "    3 : 'happy',\n",
    "    4 : 'sad',\n",
    "    5 : 'anger',\n",
    "    6 : 'fear',\n",
    "    7 : 'disgust',\n",
    "    8 : 'suprise'\n",
    "\n",
    "}\n",
    "\n",
    "ravdess_df['emotions'] = ravdess_df['emotions'].map(emotion_map)\n",
    "\n",
    "print(ravdess_df.head())\n",
    "\n",
    "print(ravdess_df['emotions'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "emotions = []\n",
    "\n",
    "for dirname, _, filenames in os.walk(tess): # (dirname, subdirs, filenames)\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.wav'):\n",
    "            paths.append(os.path.join(dirname, filename))\n",
    "            emotion = filename.split('.')[0].split('_')[2]  # Get the emotion code (e.g., 'ANG')\n",
    "            emotions.append(emotion)\n",
    "\n",
    "print(paths[:5])\n",
    "\n",
    "print(emotions[:5])\n",
    "\n",
    "# Create DataFrame\n",
    "tess_df = pd.DataFrame()\n",
    "tess_df['paths'] = paths\n",
    "tess_df['emotions'] = emotions\n",
    "\n",
    "# Map emotion codes to full emotions\n",
    "emotion_map = {\n",
    "    'angry': 'anger',\n",
    "    'disgust': 'disgust',\n",
    "    'fear': 'fear',\n",
    "    'happy': 'happy',\n",
    "    'neutral': 'neutral',\n",
    "    'ps' : 'suprise',\n",
    "    'sad' : 'sad'\n",
    "}\n",
    "\n",
    "tess_df['emotions'] = tess_df['emotions'].map(emotion_map)\n",
    "\n",
    "print(tess_df.head())\n",
    "\n",
    "print(tess_df['emotions'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "emotions = []\n",
    "\n",
    "for dirname, _, filenames in os.walk(savee): # (dirname, subdirs, filenames)\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.wav'):\n",
    "            paths.append(os.path.join(dirname, filename))\n",
    "            part = filename.split('_')[1]  # Get the emotion code (e.g., 'ANG')\n",
    "            emotion = part[:-6]\n",
    "            emotions.append(emotion)\n",
    "\n",
    "print(paths[:5])\n",
    "\n",
    "print(emotions[:5])\n",
    "\n",
    "# Create DataFrame\n",
    "savee_df = pd.DataFrame()\n",
    "savee_df['paths'] = paths\n",
    "savee_df['emotions'] = emotions\n",
    "\n",
    "emotion_map = {\n",
    "    'n': 'neutral',\n",
    "    'd': 'disgust',\n",
    "    'a': 'anger',\n",
    "    'f': 'fear',\n",
    "    'h': 'happy',\n",
    "    'sa': 'sad',\n",
    "    'su' : 'suprise'\n",
    "}\n",
    "\n",
    "savee_df['emotions'] = savee_df['emotions'].map(emotion_map)\n",
    "\n",
    "print(savee_df.head())\n",
    "\n",
    "print(savee_df['emotions'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_data = pd.concat([cremad_df, ravdess_df, tess_df, savee_df], axis = 0)\n",
    "\n",
    "emotion_data.to_csv(\"emotion_data.csv\", index=False)\n",
    "\n",
    "print(emotion_data.emotions.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.title('Emotions Count', size=16)\n",
    "sns.countplot(emotion_data.emotions)\n",
    "plt.xlabel('Count', size=12)\n",
    "plt.ylabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sr = librosa.load(paths[0], sr=None) #Latest path value from SAVEE (angry)\n",
    "print(emotions[0])\n",
    "ipd.Audio(data,rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 128\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "fmax = 8000\n",
    "mel_spectogram = librosa.feature.melspectrogram(y=data, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, fmax=fmax)\n",
    "\n",
    "log_mel_spectogram = librosa.power_to_db(mel_spectogram)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.specshow(log_mel_spectogram, x_axis='time', y_axis='mel', sr=sr)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel Spectogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=30)\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.subplot(3,1,1)\n",
    "librosa.display.specshow(mfcc, x_axis='time')\n",
    "plt.ylabel('MFCC')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Stretching\n",
    "def stretch(audio, rate=0.8):\n",
    "    return librosa.effects.time_stretch(audio, rate=0.8)\n",
    "\n",
    "# Pitch Shifting\n",
    "def pitch(audio, sr):\n",
    "    return librosa.effects.pitch_shift(y=audio, sr=sr, n_steps=2)\n",
    "\n",
    "# Add Noise\n",
    "def noise(audio, noise_level=0.005):\n",
    "    noise_amp = noise_level * np.amax(data)\n",
    "    audio = audio + noise_amp * np.random.normal(0, 1, len(data))\n",
    "    return audio\n",
    "\n",
    "# Shifting (Time warping)\n",
    "def shift(audio):\n",
    "    return np.roll(audio, shift=int(sr * 0.2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal\n",
    "import librosa.display\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.waveshow(y=data, sr=sr)\n",
    "ipd.Audio(data,rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Stretching\n",
    "x = stretch(data)\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitch Shifting\n",
    "x = pitch(data, sr)\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Noise\n",
    "x = noise(data)\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shifting (Time warping)\n",
    "x = shift(data)\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
