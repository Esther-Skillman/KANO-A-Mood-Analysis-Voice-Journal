{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databased used\n",
    "- CREMA-D: 7,442 clips\n",
    "- RAVDESS: 2,880 clips\n",
    "- TESS: 5,600 clips\n",
    "- SAVEE: 480 clips\n",
    "\n",
    "This provides a total of **16,402** inputs overall, with 7 emotions (anger, disgust, fear, happy, neutral, sad, suprise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# KERNEL SETUP IN VS CODE:\n",
    "# conda create -n myenv python=3.12.2\n",
    "# conda activate myenv\n",
    "\n",
    "%pip install resampy tf_keras tensorflow librosa pandas matplotlib kagglehub seaborn\n",
    "\n",
    "import IPython.display as ipd\n",
    "from IPython.display import Audio\n",
    "import kagglehub\n",
    "import librosa\n",
    "from librosa import feature\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets via kagglehub\n",
    "\n",
    "cremad = kagglehub.dataset_download(\"ejlok1/cremad\")\n",
    "print(\"CREMA-D to dataset files:\", cremad)\n",
    "\n",
    "ravdess = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
    "print(\"RAVDESS to dataset files:\", ravdess)\n",
    "\n",
    "tess = kagglehub.dataset_download(\"ejlok1/toronto-emotional-speech-set-tess\")\n",
    "print(\"RAVDESS to dataset files:\", tess)\n",
    "\n",
    "savee = kagglehub.dataset_download(\"ejlok1/surrey-audiovisual-expressed-emotion-savee\")\n",
    "print(\"RAVDESS to dataset files:\", savee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "## CREMA-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CREMA-D Dataset\n",
    "paths = []\n",
    "emotions = []\n",
    "\n",
    "for dirname, _, filenames in os.walk(cremad): # (dirname, subdirs, filenames)\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.wav'):\n",
    "            paths.append(os.path.join(dirname, filename))\n",
    "            emotion = filename.split('_')[2]  # Get the emotion code (e.g., 'ANG')\n",
    "            emotions.append(emotion)\n",
    "\n",
    "print(paths[:5])\n",
    "\n",
    "print(emotions[:5])\n",
    "\n",
    "# Create DataFrame\n",
    "cremad_df = pd.DataFrame()\n",
    "cremad_df['paths'] = paths\n",
    "cremad_df['emotions'] = emotions\n",
    "\n",
    "# Map emotion codes to full emotions\n",
    "emotion_map = {\n",
    "    'ANG': 'anger',\n",
    "    'DIS': 'disgust',\n",
    "    'FEA': 'fear',\n",
    "    'HAP': 'happy',\n",
    "    'NEU': 'neutral',\n",
    "    'SAD': 'sad'\n",
    "}\n",
    "\n",
    "cremad_df['emotions'] = cremad_df['emotions'].map(emotion_map)\n",
    "\n",
    "print(cremad_df.head())\n",
    "\n",
    "print(cremad_df['emotions'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAVEDESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "emotions = []\n",
    "\n",
    "for dirname, _, filenames in os.walk(ravdess): # (dirname, subdirs, filenames)\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.wav'):\n",
    "            paths.append(os.path.join(dirname, filename))\n",
    "            part = filename.split('.')[0].split('-')  # Get the emotion number (e.g., '03' = happy)\n",
    "            emotions.append(int(part[2]))\n",
    "\n",
    "\n",
    "print(paths[:5])\n",
    "print(emotions[:5])\n",
    "\n",
    "# Create DataFrame\n",
    "ravdess_df = pd.DataFrame()\n",
    "ravdess_df['paths'] = paths\n",
    "ravdess_df['emotions'] = emotions\n",
    "\n",
    "\n",
    "# Map emotion codes to full emotions\n",
    "emotion_map = {\n",
    "    1 : 'neutral',\n",
    "    2 : 'neutral', # calm as neutral to balance dataset\n",
    "    3 : 'happy',\n",
    "    4 : 'sad',\n",
    "    5 : 'anger',\n",
    "    6 : 'fear',\n",
    "    7 : 'disgust',\n",
    "    8 : 'suprise'\n",
    "\n",
    "}\n",
    "\n",
    "ravdess_df['emotions'] = ravdess_df['emotions'].map(emotion_map)\n",
    "\n",
    "print(ravdess_df.head())\n",
    "\n",
    "print(ravdess_df['emotions'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "emotions = []\n",
    "\n",
    "for dirname, _, filenames in os.walk(tess): # (dirname, subdirs, filenames)\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.wav'):\n",
    "            paths.append(os.path.join(dirname, filename))\n",
    "            emotion = filename.split('.')[0].split('_')[2]  # Get the emotion code (e.g., 'ANG')\n",
    "            emotions.append(emotion)\n",
    "\n",
    "print(paths[:5])\n",
    "\n",
    "print(emotions[:5])\n",
    "\n",
    "# Create DataFrame\n",
    "tess_df = pd.DataFrame()\n",
    "tess_df['paths'] = paths\n",
    "tess_df['emotions'] = emotions\n",
    "\n",
    "# Map emotion codes to full emotions\n",
    "emotion_map = {\n",
    "    'angry': 'anger',\n",
    "    'disgust': 'disgust',\n",
    "    'fear': 'fear',\n",
    "    'happy': 'happy',\n",
    "    'neutral': 'neutral',\n",
    "    'ps' : 'suprise',\n",
    "    'sad' : 'sad'\n",
    "}\n",
    "\n",
    "tess_df['emotions'] = tess_df['emotions'].map(emotion_map)\n",
    "\n",
    "print(tess_df.head())\n",
    "\n",
    "print(tess_df['emotions'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "emotions = []\n",
    "\n",
    "for dirname, _, filenames in os.walk(savee): # (dirname, subdirs, filenames)\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.wav'):\n",
    "            paths.append(os.path.join(dirname, filename))\n",
    "            part = filename.split('_')[1]  # Get the emotion code (e.g., 'ANG')\n",
    "            emotion = part[:-6]\n",
    "            emotions.append(emotion)\n",
    "\n",
    "print(paths[:5])\n",
    "\n",
    "print(emotions[:5])\n",
    "\n",
    "# Create DataFrame\n",
    "savee_df = pd.DataFrame()\n",
    "savee_df['paths'] = paths\n",
    "savee_df['emotions'] = emotions\n",
    "\n",
    "emotion_map = {\n",
    "    'n': 'neutral',\n",
    "    'd': 'disgust',\n",
    "    'a': 'anger',\n",
    "    'f': 'fear',\n",
    "    'h': 'happy',\n",
    "    'sa': 'sad',\n",
    "    'su' : 'suprise'\n",
    "}\n",
    "\n",
    "savee_df['emotions'] = savee_df['emotions'].map(emotion_map)\n",
    "\n",
    "print(savee_df.head())\n",
    "\n",
    "print(savee_df['emotions'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_data = pd.concat([cremad_df, ravdess_df, tess_df, savee_df], axis = 0)\n",
    "\n",
    "emotion_data.to_csv(\"emotion_data.csv\", index=False)\n",
    "\n",
    "print(emotion_data.emotions.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.title('Emotions Count', size=16)\n",
    "sns.countplot(emotion_data.emotions)\n",
    "plt.xlabel('Count', size=12)\n",
    "plt.ylabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sr = librosa.load(paths[0], sr=None) #Latest path value from SAVEE (angry)\n",
    "print(emotions[0])\n",
    "ipd.Audio(data,rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 128\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "fmax = 8000\n",
    "mel_spectogram = librosa.feature.melspectrogram(y=data, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, fmax=fmax)\n",
    "\n",
    "log_mel_spectogram = librosa.power_to_db(mel_spectogram)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.specshow(log_mel_spectogram, x_axis='time', y_axis='mel', sr=sr)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel Spectogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=30)\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.subplot(3,1,1)\n",
    "librosa.display.specshow(mfcc, x_axis='time')\n",
    "plt.ylabel('MFCC')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Stretching\n",
    "def stretch(audio, rate=0.8):\n",
    "    return librosa.effects.time_stretch(audio, rate=0.8)\n",
    "\n",
    "# Pitch Shifting\n",
    "def pitch(audio, sr):\n",
    "    return librosa.effects.pitch_shift(y=audio, sr=sr, n_steps=2)\n",
    "\n",
    "# Add Noise\n",
    "def noise(audio, noise_level=0.005):\n",
    "    noise_amp = noise_level * np.amax(audio)\n",
    "    noise_array = noise_amp * np.random.normal(0, 1, len(audio))\n",
    "    audio_with_noise = audio + noise_array\n",
    "    return audio_with_noise\n",
    "\n",
    "# Shifting (Time warping)\n",
    "def shift(audio):\n",
    "    return np.roll(audio, shift=int(sr * 0.2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal\n",
    "import librosa.display\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.waveshow(y=data, sr=sr)\n",
    "ipd.Audio(data,rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Stretching\n",
    "x = stretch(data)\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitch Shifting\n",
    "x = pitch(data, sr)\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Noise\n",
    "x = noise(data)\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shifting (Time warping)\n",
    "x = shift(data)\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 128\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "fmax = 8000\n",
    "\n",
    "def mel_spectogram(data, sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=data, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, fmax=fmax)\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max) # =np.max normalised relative to the most intense value\n",
    "    return log_mel_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(path,duration=2.5, offset=0.6):\n",
    "    data, sr =librosa.load(path,duration=duration,offset=offset)\n",
    "\n",
    "    features = []\n",
    "\n",
    "    aud=mel_spectogram(data, sr)\n",
    "    features.append(aud)\n",
    "\n",
    "    pitched_aud = pitch(data, sr)\n",
    "    pitch_ms = mel_spectogram(pitched_aud, sr)\n",
    "    features.append(pitch_ms)\n",
    "\n",
    "    noised_aud = noise(data)\n",
    "    noise_ms = mel_spectogram(noised_aud, sr)\n",
    "    features.append(noise_ms)\n",
    "\n",
    "    shifted_aud = noise(data)\n",
    "    shift_ms = mel_spectogram(shifted_aud, sr)\n",
    "    features.append(shift_ms)\n",
    "\n",
    "    pitched_audN = pitched_aud\n",
    "    pitch_noise_aud = noise(pitched_audN)\n",
    "    pitch_noise_mc = mel_spectogram(pitch_noise_aud, sr)\n",
    "    features.append(pitch_noise_mc)\n",
    "\n",
    "    features = np.array([spec[..., np.newaxis] for spec in features])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "print(\"Number of processors: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 audio has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "502it [00:43, 13.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 audio has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1002it [01:27, 11.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 audio has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1502it [02:08, 11.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 audio has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2002it [02:52, 10.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 audio has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2502it [03:36, 10.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 audio has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3002it [04:21,  9.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 audio has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3501it [05:07, 11.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500 audio has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [05:54, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 audio has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4301it [06:23, 10.82it/s]"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical \n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "X, Y=[],[]\n",
    "\n",
    "for path, emotion, index in tqdm (zip(emotion_data.paths, emotion_data.emotions, range(emotion_data.paths.shape[0]))):\n",
    "    if index%500==0:\n",
    "        print(f'{index} audio has been processed')\n",
    "\n",
    "    features=get_features(path)\n",
    "\n",
    "    for feature in features:\n",
    "\n",
    "        feature = feature[..., np.newaxis]\n",
    "\n",
    "        X.append(feature)\n",
    "        Y.append(emotion)\n",
    "\n",
    "print('Done')\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "print(X.head())\n",
    "print(Y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_map = {'anger': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'suprise': 6}\n",
    "Y = np.array([emotion_map[emotion] for emotion in Y])\n",
    "Y = to_categorical(Y, num_classes=len(emotion_map))\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of Y: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X))\n",
    "print(X[:5])\n",
    "print(Y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
